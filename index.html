<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Yuan Gao</title>
  
  <meta name="author" content="Yuan Gao">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <style>
        name {
            font-size: 32px;  /* 放大字体 */
            font-weight: bold; /* 加粗 */
            display: block;    /* 确保样式生效 */
        }
        .circle-image {
            width: 220px;
            height: 220px;
            border-radius: 50%;
            object-fit: cover;
            overflow: hidden;
            border: 2px solid #ccc;
        }

        a {
            display: inline-block;
        }
	  
	    a {
    text-decoration: none;
    color: #0366d6;
  }


        .publication-item {
            margin-bottom: 20px;
            padding: 15px;
            border-left: 3px solid #ddd;
            background-color: #f9f9f9;
        }

        .publication-title {
            font-weight: bold;
            font-size: 16px;
            margin-bottom: 8px;
            color: #333;
        }

        .publication-authors {
            margin-bottom: 5px;
            color: #666;
        }

        .publication-venue {
            font-style: italic;
            margin-bottom: 8px;
            color: #007acc;
        }

        .publication-description {
            color: #555;
            line-height: 1.4;
        }

        .year-header {
            text-align: left;
            font-weight: bold;
            font-size: 20px;
            font-style: italic;
            padding-bottom: 20px;
            padding-top: 5px;
            border-bottom: 2px solid #ddd;
            margin-bottom: 20px;
        }
    </style>
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name> Yuan Gao（高远） </name>
              </p>
              <p>
				 I'm now a <strong>research engineer in medical AI</strong> at Alibaba Damo Academy. Before the job, I was a <strong>postdoctoral research fellow</strong> in the Biomedical Image Analysis LAB, IBME, University of Oxford.
				</p>
				<p>
				 I hold an MSc and a DPhil in Biomedical Engineering from the University of Oxford, supervised by <a href="https://eng.ox.ac.uk/people/vicente-grau-colomer/">Prof. Vicente Grau</a> and <a href="https://www.eng.ox.ac.uk/people/alison-noble/">Prof. Alison Noble</a>, respectively.
				</p>
				<p>
				 My research focuses on <strong>medical image analysis and deep learning</strong>. Specifically, I study the fetal ultrasound video and build smart agents for navigating scans automatically, assisting decision-making in diagnosis.
				</p>
				<p>
				 I am experienced in creating machine learning solutions and applying in solving real-world applications.
			  <p>
                <a href="mailto:sgygao2@gmail.com"> Email </a> &nbsp;/&nbsp;
                <a href=" https://scholar.google.com/citations?hl=en&user=1Tvfo7wAAAAJ&view_op=list_works"> Google Scholar </a> &nbsp;/&nbsp;
                <a href="https://linkedin.com/in/yuan-gao-ab804a83/"> LinkedIn </a> &nbsp;/&nbsp;
				<a href="https://github.com/YGOX/"> Github </a> 
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/avatar.jpg"><img class="circle-image" alt="profile photo" src="images/avatar.jpg"></a>
            </td>
          </tr>
        </tbody></table>
	<br>

		



        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td>
              <heading style="font-size: 24px; margin-top: 400px; margin-bottom: 0px;"><b>Research</b></heading>
                <br/><p></p>

              <div class="year-header">2025</div>
              
              <div class="publication-item">
                <div class="publication-title">
                  <a href="https://www.researchsquare.com/article/rs-5783643/v1">Multi-modal AI for Opportunistic Screening, Staging and Progression Risk Stratification of Steatotic Liver Disease</a>
                </div>
                <div class="publication-authors">
                  <strong>Yuan Gao</strong>, Chun Li Li, Wan Xing Chang, Bai Du, Xianghua Ye, Ying Da Xia, Heng Guo, Xiao Ming Zhang, Wei Liu, Ruo Bing Bai, Bei Bei Li, Yang Hong, Jiawen Yao, Le Lu, Kai Cao, Yee Hui Yeo, Jun Chen, Jie Li, Ke Yan, Yang Hou, Ling Zhang, Yu Shi
                </div>
                <div class="publication-venue">Nature communications, In Press, 2025</div>
                <div class="publication-description">
                  In this paper, we 
				</div>
              </div>

				
              <div class="publication-item">
                <div class="publication-title">
                  <a href="https://arxiv.org/pdf/2507.03872">PLUS: Plug-and-play enhanced liver lesion diagnosis model on non-contrast CT scans</a>
                </div>
                <div class="publication-authors">
                  Jiacheng Hao, Xiaoming Zhang, Wei Liu, Xiaoli Yin, <strong>Yuan Gao</strong>, Chunli Li, Ling Zhang, Le Lu, Yu Shi, Xu Han, Ke Yan
                </div>
                <div class="publication-venue">MICCAI, 2025</div>
                <div class="publication-description">
                  We present ....
				</div>
              </div>

				
              <div class="year-header">2024</div>
				
              <div class="publication-item">
                <div class="publication-title">
                    <a href="https://arxiv.org/pdf/2407.13210?">Improved Esophageal Varices Assessment from Non-Contrast CT Scans</a>
                </div>
                <div class="publication-authors">
                    Chunli Li, Xiaoming Zhang, <strong>Yuan Gao</strong>, Xiaoli Yin, Le Lu, Ling Zhang, Ke Yan, Yu Shi
                </div>
                <div class="publication-venue">MICCAI, 2024</div>
                <div class="publication-description">
                    We present ....
			</div>
            </div>

              <div class="publication-item">
                <div class="publication-title">
                  <a href="https://arxiv.org/pdf/2407.13217">LIDIA: Precise Liver Tumor Diagnosis on Multi-Phase Contrast-Enhanced CT via Iterative Fusion and Asymmetric Contrastive Learning</a>
                </div>
                <div class="publication-authors">
                  Wei Huang, Wei Liu, Xiaoming Zhang, Xiaoli Yin, Xu Han, Chunli Li, <strong>Yuan Gao</strong>, Yu Shi, Le Lu, Ling Zhang, Lei Zhang, Ke Yan
                </div>
                <div class="publication-venue">MICCAI, 2024</div>
                <div class="publication-description">
                  In this study, we ...
				</div>
              </div>


              <div class="publication-item">
                  <div class="publication-title">
                      <a href="https://ieeexplore.ieee.org/abstract/document/10632195">RemixFormer++: A Multi-modal Transformer Model for Precision Skin Tumor Differential Diagnosis with Memory-efficient Attention</a>
                  </div>
                  <div class="publication-authors">
                      Jing Xu, Kai Huang, Lianzhen Zhong, <strong>Yuan Gao</strong>, Kai Sun, Wei Liu, Yanjie Zhou, Wenchao Guo, Yuan Guo, Yuanqiang Zou, Yuping Duan, Le Lu, Yu Wang, Xiang Chen, Shuang Zhao
                  </div>
                  <div class="publication-venue">TMI, 2024</div>
                  <div class="publication-description">
                      In this study, we 
				  </div>
              </div>

              

              <div class="year-header">2023</div>

              <!-- <div class="publication-item">
                  <div class="publication-title">
                      <a href="https://arxiv.org/abs/2310.09909">Can GPT-4V(ision) Serve Medical Applications? Case Studies on GPT-4V for Multimodal Medical Diagnosis</a>
                  </div>
                  <div class="publication-authors">
                      <strong>Chaoyi Wu*</strong>, Jiayu Lei*, Qiaoyu Zheng*, Weike Zhao*, Weixiong Lin*, Xiaoman Zhang*, Xiao Zhou*, Ziheng Zhao*, Yanfeng Wang, Ya Zhang, Weidi Xie
                  </div>
                  <div class="publication-venue">Technical Report, 2023</div>
                  <div class="publication-description">
                      We evaluate the GPT-4V on 92 radiographic cases, 20 pathology cases and 16 location cases across 17 medical systems covering 8 imaging modalities. In general, as the cases shown, GPT-4V is still far from clinical usage.
                  </div>
              </div> -->

              <!-- <div class="publication-item">
                  <div class="publication-title">
                      <a href="https://arxiv.org/pdf/2309.06828.pdf">UniBrain: Universal Brain MRI Diagnosis with Hierarchical Knowledge-enhanced Pre-training</a>
                  </div>
                  <div class="publication-authors">
                      Jiayu Lei, Lisong Dai, Haoyun Jiang, <strong>Chaoyi Wu</strong>, Xiaoman Zhang, Yao Zhang, Jiangchao Yao, Weidi Xie, Yanyong Zhang, Yuehua Li, Ya Zhang, Yanfeng Wang
                  </div>
                  <div class="publication-venue">Computerized Medical Imaging and Graphics (CMIG), 2025</div>
                  <div class="publication-description">
                      We release a new knowledge-enhanced Brain MRI pre-train foundation model leveraging image-report pairs which can realize zero-shot diagnosis of unseen brain diseases.
                  </div>
              </div> -->

              <!-- <div class="publication-item">
                  <div class="publication-title">
                      <a href="https://arxiv.org/abs/2308.02463">Towards Generalist Foundation Model for Radiology by Leveraging Web-scale 2D&3D Medical Data</a>
                  </div>
                  <div class="publication-authors">
                      <strong>Chaoyi Wu*</strong>, Xiaoman Zhang*, Yanfeng Wang, Ya Zhang, Weidi Xie
                  </div>
                  <div class="publication-venue">Nature Communications</div>
                  <div class="publication-description">
                      In this study, we aim to initiate the development of Radiology Foundation Model, termed as RadFM. We construct a large-scale Medical Multi-modal Dataset, MedMD, consisting of 16M 2D and 3D medical scans.
                  </div>
              </div> -->

              <!-- <div class="publication-item">
                  <div class="publication-title">
                      <a href="https://arxiv.org/pdf/2305.10415.pdf">Development of a large-scale medical visual question-answering dataset</a>
                  </div>
                  <div class="publication-authors">
                      Xiaoman Zhang*, <strong>Chaoyi Wu*</strong>, Weixiong Lin, Ziheng Zhao, Yanfeng Wang, Ya Zhang, Weidi Xie
                  </div>
                  <div class="publication-venue">Nature Communications Medicine, 2024</div>
                  <div class="publication-description">
                      In this paper, we focus on the problem of Medical Visual Question Answering (MedVQA). We propose a generative medical VQA model, MedVInT, together with a large scale MedVQA Dataset, PMC-VQA.
                  </div>
              </div> -->

              <!-- <div class="publication-item">
                  <div class="publication-title">
                      <a href="https://arxiv.org/abs/2304.14454">PMC-LLaMA: Towards Building Open-source Language Models for Medicine</a>
                  </div>
                  <div class="publication-authors">
                      <strong>Chaoyi Wu</strong>, Xiaoman Zhang, Yanfeng Wang, Ya Zhang, Weidi Xie
                  </div>
                  <div class="publication-venue">Journal of the American Medical Informatics Association (JAMIA)</div>
                  <div class="publication-description">
                      In this report, we introduce PMC-LLaMA, an open-source language model that is acquired leveraging large medical corpus, surpassing chatGPT on medicalQA benchmarks.
                  </div>
              </div> -->

              <!-- <div class="publication-item">
                  <div class="publication-title">
                      <a href="https://arxiv.org/pdf/2303.07240">PMC-CLIP: Contrastive Language-Image Pre-training using Biomedical Documents</a>
                  </div>
                  <div class="publication-authors">
                      Weixiong Lin, Ziheng Zhao, Xiaoman Zhang, <strong>Chaoyi Wu</strong>, Yanfeng Wang, Ya Zhang, Weidi Xie
                  </div>
                  <div class="publication-venue">International Conference on Medical Image Computing and Computer Assisted Intervention (MICCAI), 2023</div>
                  <div class="publication-description">
                      We collect a biomedical dataset, PMC-OA with <strong>1.6M</strong> image-caption pairs collected from PubMedCentral's OpenAccess subset.
                  </div>
              </div> -->

              <!-- <div class="publication-item">
                  <div class="publication-title">
                      <a href="https://arxiv.org/abs/2302.14042">Knowledge-enhanced Pre-training for Auto-diagnosis of Chest Radiology Images</a>
                  </div>
                  <div class="publication-authors">
                      Xiaoman Zhang, <strong>Chaoyi Wu</strong>, Yanfeng Wang, Ya Zhang, Weidi Xie
                  </div>
                  <div class="publication-venue">Nature Communications, 2023</div>
                  <div class="publication-description">
                      Here, we propose a knowledge-enhanced vision-language pre-training approach for auto-diagnosis on chest X-ray images. First trains a knowledge encoder based on an existing medical knowledge graph, then leverages the pre-trained knowledge encoder to guide the visual representation learning.
                  </div>
              </div> -->

              <!-- <div class="publication-item">
                  <div class="publication-title">
                      <a href="https://arxiv.org/abs/2302.11557">K-Diag: Knowledge-enhanced Disease Diagnosis in Radiographic Imaging</a>
                  </div>
                  <div class="publication-authors">
                      <strong>Chaoyi Wu*</strong>, Xiaoman Zhang*, Yanfeng Wang, Ya Zhang, Weidi Xie
                  </div>
                  <div class="publication-venue">MICCAI2023-Workshop, Oral</div>
                  <div class="publication-description">
                      In this paper, we consider the problem of disease diagnosis. Unlike the conventional learning paradigm that treats labels independently, we propose a knowledge-enhanced framework, that enables training visual representation with the guidance of medical domain knowledge.
                  </div>
              </div> -->

              <!-- <div class="publication-item">
                  <div class="publication-title">
                      <a href="https://arxiv.org/abs/2301.02228">MedKLIP: Medical Knowledge Enhanced Language-Image Pre-Training</a>
                  </div>
                  <div class="publication-authors">
                      <strong>Chaoyi Wu</strong>, Xiaoman Zhang, Ya Zhang, Yanfeng Wang, Weidi Xie
                  </div>
                  <div class="publication-venue">International Conference on Computer Vision (ICCV), 2023</div>
                  <div class="publication-description">
                      We propose to leverage medical specific knowledge enhancing language-image pre-training method, significantly advancing the ability of pre-trained models to handle unseen diseases on zero-shot classification and grounding tasks.
                  </div>
              </div> -->

              <div class="year-header">2022</div>

              <!-- <div class="publication-item">
                  <div class="publication-title">
                      <a href="https://link.springer.com/chapter/10.1007/978-3-031-16431-6_2">Boundary-Enhanced Self-supervised Learning for Brain Structure Segmentation</a>
                  </div>
                  <div class="publication-authors">
                      Feng Chang, <strong>Chaoyi Wu</strong>, Yanfeng Wang, Ya Zhang, Xin Chen, Qi Tian
                  </div>
                  <div class="publication-venue">International Conference on Medical Image Computing and Computer Assisted Intervention (MICCAI), 2022</div>
                  <div class="publication-description">
                      We propose Boundary-Enhanced Self-Supervised Learning (BE-SSL), leveraging supervoxel segmentation and registration as two related proxy tasks, enhancing brain structure segmentation.
                  </div>
              </div> -->

              <!-- <div class="publication-item">
                  <div class="publication-title">
                      <a href="https://www.sciencedirect.com/science/article/pii/S0895611122000805">Integrating features from lymph node stations for metastatic lymph node detection</a>
                  </div>
                  <div class="publication-authors">
                      <strong>Chaoyi Wu</strong>, Feng Chang, Xiao Su, Zhihan Wu, Yanfeng Wang, Ling Zhu, Ya Zhang
                  </div>
                  <div class="publication-venue">Computerized Medical Imaging and Graphics (CMIG), 2022, 101: 102108</div>
                  <div class="publication-description">
                      We first leverage the information of LN stations for metastatic LN detection. Metastatic LN station classification is proposed as proxy task for metastatic LN detection. A GCN-based structure is adopted to model the mutual influence among LN stations.
                  </div>
              </div> -->
            </td>
          </tr>
        </tbody></table>

        <!-- <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
		Based on a template by <a href="https://github.com/jonbarron/jonbarron_website">Jon Barron</a>.
                <br>
              </p>
            </td>
          </tr> -->

		  
        </tbody>
		</table>
      </td>
    </tr>
  </table>
</body>

</html>

       
