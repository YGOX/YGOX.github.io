<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Yuan Gao</title>
  
  <meta name="author" content="Yuan Gao">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <style>
        name {
            font-size: 32px;  /* 放大字体 */
            font-weight: bold; /* 加粗 */
            display: block;    /* 确保样式生效 */
        }
        .circle-image {
            width: 220px;
            height: 220px;
            border-radius: 50%;
            object-fit: cover;
            overflow: hidden;
            border: 2px solid #ccc;
        }

        a {
            display: inline-block;
        }
	  
	    a {
    text-decoration: none;
    color: #0366d6;
  }


        .publication-item {
            margin-bottom: 20px;
            padding: 15px;
            border-left: 3px solid #ddd;
            background-color: #f9f9f9;
        }

        .publication-title {
            font-weight: bold;
            font-size: 16px;
            margin-bottom: 8px;
            color: #333;
        }

        .publication-authors {
            margin-bottom: 5px;
            color: #666;
        }

        .publication-venue {
            font-style: italic;
            margin-bottom: 8px;
            color: #007acc;
        }

        .publication-description {
            color: #555;
            line-height: 1.4;
        }

        .year-header {
            text-align: left;
            font-weight: bold;
            font-size: 20px;
            font-style: italic;
            padding-bottom: 20px;
            padding-top: 5px;
            border-bottom: 2px solid #ddd;
            margin-bottom: 20px;
        }
    </style>
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name> Yuan Gao（高远） </name>
              </p>
              <p>
				 I'm now a <strong>research engineer in medical AI</strong> at Alibaba Damo Academy. Before the job, I was a <strong>postdoctoral research fellow</strong> in the Biomedical Image Analysis LAB, IBME, University of Oxford.
				</p>
				<p>
				 I hold an MSc and a DPhil in <strong>Biomedical Engineering</strong> from the University of Oxford, supervised by <a href="https://eng.ox.ac.uk/people/vicente-grau-colomer/">Prof. Vicente Grau</a> and <a href="https://www.eng.ox.ac.uk/people/alison-noble/">Prof. Alison Noble</a>, respectively.
				</p>
				<p>
				 My research focuses on <strong>medical image analysis and deep learning</strong>. Specifically, I study the fetal ultrasound video and build smart agents for navigating scans automatically, assisting decision-making in diagnosis.
				</p>
				<p>
				 I am experienced in creating machine learning solutions and applying in solving real-world applications.
			  <p>
                <a href="mailto:sgygao2@gmail.com"> Email </a> &nbsp;/&nbsp;
                <a href=" https://scholar.google.com/citations?hl=en&user=1Tvfo7wAAAAJ&view_op=list_works"> Google Scholar </a> &nbsp;/&nbsp;
                <a href="https://linkedin.com/in/yuan-gao-ab804a83/"> LinkedIn </a> &nbsp;/&nbsp;
				<a href="https://github.com/YGOX/"> Github </a> 
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/avatar.jpg"><img class="circle-image" alt="profile photo" src="images/avatar.jpg"></a>
            </td>
          </tr>
        </tbody></table>
	<br>

		



        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td>
              <heading style="font-size: 24px; margin-top: 400px; margin-bottom: 0px;"><b>Selected Research Contributions </b></heading>
                <br/><p></p>
				

        <div class="publication-item" style="display: flex; margin-bottom: 24px; align-items: center;">
          <img src="images/pic1.png" alt="MAOSS" style="width: 180px; height: auto; margin-right: 16px; border-radius: 4px; object-fit: cover;">
          <div>
            <div class="publication-title">
              <a href="https://www.researchsquare.com/article/rs-5783643/v1">Multi-modal AI for Opportunistic Screening, Staging and Progression Risk Stratification of Steatotic Liver Disease</a>
            </div>
            <div class="publication-authors">
              <strong>Yuan Gao</strong>, Chun Li Li, Wan Xing Chang, Bai Du, Xianghua Ye, Ying Da Xia, Heng Guo, Xiao Ming Zhang, Wei Liu, Ruo Bing Bai, Bei Bei Li, Yang Hong, Jiawen Yao, Le Lu, Kai Cao, Yee Hui Yeo, Jun Chen, Jie Li, Ke Yan, Yang Hou, Ling Zhang, Yu Shi
            </div>
            <div class="publication-venue">Nature communications, In Press, 2025</div>
            <div class="publication-description">
              We propose MAOSS, a multimodal AI for opportunistic steatotic liver disease (SLD) screening using non-contrast CT, serum data, and demographics. Trained on 1,783 cases and validated across >20,000 patients, it detects ≥S1 steatosis with AUCs of 0.903–0.929, significantly improving radiologists’ accuracy (AUC: 0.709→0.798, p<0.001) and identifying 31% more high-risk patients in simulated screening—offering a scalable tool for early SLD detection.
            </div>
          </div>
        </div>
				
              
              <!-- <div class="publication-item">
                <div class="publication-title">
                  <a href="https://www.researchsquare.com/article/rs-5783643/v1">Multi-modal AI for Opportunistic Screening, Staging and Progression Risk Stratification of Steatotic Liver Disease</a>
                </div>
                <div class="publication-authors">
                  <strong>Yuan Gao</strong>, Chun Li Li, Wan Xing Chang, Bai Du, Xianghua Ye, Ying Da Xia, Heng Guo, Xiao Ming Zhang, Wei Liu, Ruo Bing Bai, Bei Bei Li, Yang Hong, Jiawen Yao, Le Lu, Kai Cao, Yee Hui Yeo, Jun Chen, Jie Li, Ke Yan, Yang Hou, Ling Zhang, Yu Shi
                </div>
                <div class="publication-venue">Nature communications, In Press, 2025</div>
                <div class="publication-description">
                  We propose MAOSS, a multimodal AI for opportunistic steatotic liver disease (SLD) screening using non-contrast CT, serum data, and demographics. Trained on 1,783 cases and validated across >20,000 patients, it detects ≥S1 steatosis with AUCs of 0.903–0.929, significantly improving radiologists’ accuracy (AUC: 0.709→0.798, p<0.001) and identifying 31% more high-risk patients in simulated screening—offering a scalable tool for early SLD detection.
				</div>
              </div> -->

				
              <!-- <div class="publication-item">
                <div class="publication-title">
                  <a href="https://arxiv.org/pdf/2507.03872">PLUS: Plug-and-Play Enhanced Liver Lesion Diagnosis Model on Non-Contrast CT Scans</a>
                </div>
                <div class="publication-authors">
                  Jiacheng Hao, Xiaoming Zhang, Wei Liu, Xiaoli Yin, <strong>Yuan Gao</strong>, Chunli Li, Ling Zhang, Le Lu, Yu Shi, Xu Han, Ke Yan
                </div>
                <div class="publication-venue">MICCAI, 2025</div>
                <div class="publication-description">
                  We present...
			</div>
              </div> -->

				
              <!-- <div class="year-header">2024</div> -->
				
            <!-- <div class="publication-item" >
                <div class="publication-title">
                    <a href="https://arxiv.org/pdf/2407.13210?">Improved Esophageal Varices Assessment from Non-Contrast CT Scans</a>
                </div>
                <div class="publication-authors">
                    Chunli Li, Xiaoming Zhang, <strong>Yuan Gao</strong>, Xiaoli Yin, Le Lu, Ling Zhang, Ke Yan, Yu Shi
                </div>
                <div class="publication-venue">MICCAI, 2024</div>
                <div class="publication-description">
                    We present MOON, a novel AI framework for non-invasive esophageal varices (EV) assessment using non-contrast CT. Inspired by radiologists’ holistic evaluation, MOON jointly analyzes the esophagus, liver, and spleen, achieving superior accuracy (AUC: 0.864 vs. 0.803 for severe EV) on a dataset of 1,255 endoscopy-confirmed cases. MOON offers the first synchronized multi-organ NC-CT approach, providing a minimally invasive alternative to endoscopy.
			</div>
            </div> -->
				
        <div class="publication-item" style="display: flex; margin-bottom: 24px; align-items: center;">
          <img src="images/pic2.png" alt="MOON" style="width: 180px; height: auto; margin-right: 16px; border-radius: 4px; object-fit: cover;">
          <div>
            <div class="publication-title">
                <a href="https://arxiv.org/pdf/2407.13210">Improved Esophageal Varices Assessment from Non-Contrast CT Scans</a>
            </div>
            <div class="publication-authors">
                Chunli Li, Xiaoming Zhang, <strong>Yuan Gao</strong>, Xiaoli Yin, Le Lu, Ling Zhang, Ke Yan, Yu Shi
            </div>
            <div class="publication-venue">MICCAI, 2024</div>
            <div class="publication-description">
                We present MOON, a novel AI framework for non-invasive esophageal varices (EV) assessment using non-contrast CT. Inspired by radiologists’ holistic evaluation, MOON jointly analyzes the esophagus, liver, and spleen, achieving superior accuracy (AUC: 0.864 vs. 0.803 for severe EV) on a dataset of 1,255 endoscopy-confirmed cases. MOON offers the first synchronized multi-organ NC-CT approach, providing a minimally invasive alternative to endoscopy.
            </div>
          </div>
        </div>

        <div class="publication-item" style="display: flex; margin-bottom: 24px; align-items: center;">
          <img src="images/pic3.png" alt="RemixFormer++" style="width: 180px; height: auto; margin-right: 16px; border-radius: 4px; object-fit: cover;">
  <div>
    <div class="publication-title">
      <a href="https://ieeexplore.ieee.org/abstract/document/10632195">RemixFormer++: A Multi-modal Transformer Model for Precision Skin Tumor Differential Diagnosis with Memory-efficient Attention</a>
    </div>
    <div class="publication-authors">
      Jing Xu, Kai Huang, Lianzhen Zhong, <strong>Yuan Gao</strong>, Kai Sun, Wei Liu, Yanjie Zhou, Wenchao Guo, Yuan Guo, Yuanqiang Zou, Yuping Duan, Le Lu, Yu Wang, Xiang Chen, Shuang Zhao
    </div>
    <div class="publication-venue">TMI, 2024</div>
    <div class="publication-description">
      In this study, we propose RemixFormer++, a multimodal Transformer for skin tumor diagnosis that fuses clinical images, dermoscopy, and metadata using specialized attention strategies. It achieves state-of-the-art results: +5.3% F1 on Derm7pt and 92.6% accuracy on a large in-house dataset (10,351 patients). Performance rivals or exceeds that of 191 dermatologists, demonstrating strong clinical potential.
    </div>
  </div>
</div>

			 

              <!-- <div class="publication-item" >
                <div class="publication-title">
                  <a href="https://arxiv.org/pdf/2407.13217">LIDIA: Precise Liver Tumor Diagnosis on Multi-Phase Contrast-Enhanced CT via Iterative Fusion and Asymmetric Contrastive Learning</a>
                </div>
                <div class="publication-authors">
                  Wei Huang, Wei Liu, Xiaoming Zhang, Xiaoli Yin, Xu Han, Chunli Li, <strong>Yuan Gao</strong>, Yu Shi, Le Lu, Ling Zhang, Lei Zhang, Ke Yan
                </div>
                <div class="publication-venue">MICCAI, 2024</div>
                <div class="publication-description">
                  In this study, we ...
				</div>
              </div> -->


                  <!-- <div class="publication-item">
            <div class="publication-title">
                <a href="https://ieeexplore.ieee.org/abstract/document/10632195">RemixFormer++: A Multi-modal Transformer Model for Precision Skin Tumor Differential Diagnosis with Memory-efficient Attention</a>
            </div>
            <div class="publication-authors">
                      Jing Xu, Kai Huang, Lianzhen Zhong, <strong>Yuan Gao</strong>, Kai Sun, Wei Liu, Yanjie Zhou, Wenchao Guo, Yuan Guo, Yuanqiang Zou, Yuping Duan, Le Lu, Yu Wang, Xiang Chen, Shuang Zhao
            </div>
            <div class="publication-venue">TMI, 2024</div>
            <div class="publication-description">
                      In this study, we propose RemixFormer++, a multimodal Transformer for skin tumor diagnosis that fuses clinical images, dermoscopy, and metadata using specialized attention strategies. It achieves state-of-the-art results: +5.3% F1 on Derm7pt and 92.6% accuracy on a large in-house dataset (10,351 patients). Performance rivals or exceeds that of 191 dermatologists, demonstrating strong clinical potential.
            </div>
          </div> -->
     

              

              <!-- <div class="year-header">2023</div>

              <div class="publication-item">
                  <div class="publication-title">
                      <a href="https://arxiv.org/pdf/2307.08268">Liver Tumor Screening and Diagnosis in CT with Pixel-Lesion-Patient Network</a>
                  </div>
                  <div class="publication-authors">
                      Ke Yan, Xiaoli Yin, Yingda Xia, Fakai Wang, Shu Wang, <strong>Yuan Gao</strong>, Jiawen Yao, Chunli Li, Xiaoyu Bai, Jingren Zhou, Ling Zhang, Le Lu, Yu Shi
				  </div>
                  <div class="publication-venue">MICCAI, 2023</div>
                  <div class="publication-description">
                      We evaluate ...
				  </div>
              </div> -->

<div class="publication-item" style="display: flex; margin-bottom: 24px; align-items: center;">
  <img src="images/pic4.png" alt="DermImitFormer" style="width: 180px; height: auto; margin-right: 16px; border-radius: 4px; object-fit: cover;">
  <div>
    <div class="publication-title">
      <a href="https://arxiv.org/pdf/2307.08308">A Novel Multi-Task Model Imitating Dermatologists for Accurate Differential Diagnosis of Skin Diseases in Clinical Images</a>
    </div>
    <div class="publication-authors">
      Yan-Jie Zhou, Wei Liu, <strong>Yuan Gao</strong>, Jing Xu, Le Lu, Yuping Duan, Hao Cheng, Na Jin, Xiaoyong Man, Shuang Zhao, Yu Wang
    </div>
    <div class="publication-venue">MICCAI, 2023</div>
    <div class="publication-description">
      We propose DermImitFormer, a multi-task model that mimics dermatologists’ diagnostic reasoning by jointly predicting skin diseases, body parts, and lesion attributes. It features a lesion selection module to highlight key regions and a cross-interaction module to model relationships among tasks. Evaluated on three datasets—including a new large-scale clinical set—it achieves state-of-the-art performance, enhancing both accuracy and interpretability.
    </div>
  </div>
</div>

				<!-- 
              <div class="year-header">2022</div> -->
              <div class="publication-item">
                  <div class="publication-title">
                      <a href="https://www.cs.jhu.edu/~lelu/publication/MICCAI%202022_paper1023_RemixFormer.pdf">RemixFormer: A Transformer Model for Precision Skin Tumor Differential Diagnosis via Multi-modal Imaging and Non-imaging Data</a>
                  </div>
                  <div class="publication-authors">
                      Jing Xu, <strong>Yuan Gao</strong>, Wei Liu, Kai Huang, Shuang Zhao, Le Lu, Xiaosong Wang, Xian-Sheng Hua, Yu Wang, Xiang Chen
                  </div>
                  <div class="publication-venue">MICCAI, 2022</div>
                  <div class="publication-description">
                      In this study, we propose a transformer-based multimodal framework for skin tumor diagnosis that fuses clinical images, dermoscopy, and metadata—even with missing modalities—via a novel cross-modality fusion module and disease-wise data remixing. On Derm7pt, it improves F1 by 6.5% and accuracy by 2.8%; on a large in-house dataset (5,601 patients, 10 classes), it achieves 88.5% accuracy, demonstrating robustness and clinical potential.
				  </div>
              </div>

              <div class="publication-item">
                  <div class="publication-title">
                      <a href="https://pdf.sciencedirectassets.com/272229/1-s2.0-S1568494621X00178/1-s2.0-S1568494621010966/main.pdf?X-Amz-Security-Token=IQoJb3JpZ2luX2VjECcaCXVzLWVhc3QtMSJHMEUCICjJmEQzR2lcCxKr%2FUcDhSVmWdDPMakL5MdaryEazsfEAiEAsk54A7dFTgH5HHMtPuGjTn4tnai9OCcJa0sU%2BIIbO%2FQquwUI7%2F%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FARAFGgwwNTkwMDM1NDY4NjUiDGCU5VFQMDG5ksEjTiqPBZKTL%2Fh9ne7HFyn9ylYq9wcq9W9Xur%2FvlTKateyzRePxkcDb3bOTd%2B1Rxi%2FBcum1yuve1a6Uh5mK8w7lyRVo0IMuwpSfu%2F9fp63JFO387aSRtp6HVVy1rd0L5cB1YRTugPHXczN9plo0k%2FPZ0GJ4FTB%2BsSZVrZa%2Fh0%2Fxhd9liAwvFgRQyv24RDBr7WG0Vz0CXMUEPOMv2k52WP4i7FPGPvHOjNowiTvyqUlxAD4MQHauGKvpBKCwYX%2B%2BPBOsJIRQpYRpJHuS4iGSyZYJJ49WMOe%2FyIHubf7ZJ79bHEyVuqUDoLO9jDu5HAz6wNqYL1dRewIB4cFEWRpiU6o49FaKrIqlMtBTEs62MDeopFDGypEYoOfCmHZuoNibRABjSIg%2BnAhpfJyG12XgNs%2BC8wTF0DEtGF2Te%2BwksCJ%2BrENSkKIcSPRHXuAbBul7P9A%2FORLymw%2BrdTV2F%2FGglHHAGKCW2wT0YMGcE6RtOey3jJ%2FlMoh%2BJJabTIt%2BYBs3EEj%2BwgOGVEmHOG8kcZG9qcCbO4IzdVKUXbHcySt0Kndjudrrk9Fa%2B2MBJ8xJVrdWJjy126Kif1ZXnpTnfPw3yD7YsLbiVeJC2gzJH9QnkWApYpexDVQ%2BL0JrsMmTcfnpQE%2FwLxK7z5y2gs3zAG0NQCxk3Crl61NmZJBqCGcowe30YoHTzFe1bMqHnPulJ77tLa6uc48iWZ7r1JMQjvxFh7x6AgQXRKIID4bwOpo5JzA5FhaeJpqGLxfoOv4MVJmivb0VBAE5nYsuIgIe31jhzas5nZOwhyqgN8Skos2Og%2Bt0y4aL%2BnWJMvGqhmjqUrsr4hfQq%2FaIXkgVcMOArO1G9zKz6Ha1jl2m8jfTjDbcCXPTH8T0pAkw876jygY6sQHIoRtBzNhPYDXbsW8LrqGNBMR9KrHEfjTsVHQVBS%2Fq4k5nF6xqxeH2UHtHHcD6I6AByW83t3tFNoDfWErTiRFif7ZxhJNnZUKOGqBuczwcjF3tsIT%2FHMujBN3nJHthFXmqhMf7kmn1zqx1vfmlxTARJn9w85Avl7UYinvP7qdRn%2BQdYKdjUgjvvHYxRzfI8NrirZi%2BU4WKEUEYYO%2B6YoSZGfxLykKOpSlKks8myytXVNw%3D&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20251222T072514Z&X-Amz-SignedHeaders=host&X-Amz-Expires=300&X-Amz-Credential=ASIAQ3PHCVTYQNNKYXAY%2F20251222%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Signature=c9b678e642aa2966610765cb9a503eee2e5317cb626de9d35627cebe5018372b&hash=82f970f071aafd85c270c9a8a1d9c17b921f8d29be3e31997802ac07e2df06c9&host=68042c943591013ac2b2430a89b270f6af2c76d8dfd086a07176afe7c76c2c61&pii=S1568494621010966&tid=spdf-941a3e3e-d0d2-491d-9004-8d1077572efc&sid=36b4ea346b533849600879980532cf372cc5gxrqa&type=client&tsoh=d3d3LnNjaWVuY2VkaXJlY3QuY29t&rh=d3d3LnNjaWVuY2VkaXJlY3QuY29t&ua=0e0f5700530101535750&rr=9b1de2564a99856a&cc=hk&kca=eyJrZXkiOiJMK09UdzRwbFBqVkFkQWl2R2orazRvRmt6MDVIb2VmQWlYajQwdGVLVGNHdmlSSzhSbFlJeGlrUExpRVNxTkpxazNSYTR1d2FDNnUxR215QkFTeDBRcEdMNnpIRWhtUzh2eUxmUmxCek42WEhTajhEc0JtdDdMcDhRVitJb1dhUG5kaFQxR1dleHhHbjdCUjVmelFOandtMVFBY2lBTUExMnBGODkvOWtMV3Q0SXhDUCIsIml2IjoiZWNlZDY2OWVhYWMyZjY2YmY1YzRjZDdkOGFjYTE3NjUifQ==_1766388321238">Robust weakly supervised learning for COVID-19 recognition using multi-center CT images</a>
                  </div>
                  <div class="publication-authors">
                      Qinghao Ye, <strong>Yuan Gao</strong>, Weiping Ding, Zhangming Niu, Chengjia Wang, Yinghui Jiang, Minhao Wang, Evandro Fei Fang, Wade Menpes-Smith, Jun Xia, Guang Yang
                  </div>
                  <div class="publication-venue">Applied Soft Computing, 2022</div>
                  <div class="publication-description">
                      In this paper, we propose CIFD-Net, a robust weakly supervised 3D CT recognition model for COVID-19 that addresses multi-domain shifts across hospitals and scanners. By fusing coronavirus-related information, it reliably handles appearance variations in CT scans and achieves higher accuracy than state-of-the-art methods, offering an efficient tool to alleviate radiologist workload during the pandemic.
				  </div>
              </div>


				
              <!-- <div class="year-header">2021</div> -->
              <div class="publication-item">
                  <div class="publication-title">
                      <a href="https://arxiv.org/pdf/2103.07895">Principled Ultrasound Data Augmentation for Classification of Standard Planes</a>
                  </div>
                  <div class="publication-authors">
                      Lok Hin Lee, <strong>Yuan Gao</strong>, J Alison Noble
                  </div>
                  <div class="publication-venue">IPMI, 2021</div>
                  <div class="publication-description">
                      Effective data augmentation is critical for training robust deep learning models in medical imaging, where datasets are often small. We propose a principled augmentation policy search that incorporates domain-specific and mixed-example transformations. Applied to fetal ultrasound standard plane classification, our method improves average F1-score by 7.0% over naive strategies and yields better-clustered feature representations, demonstrating the value of optimized augmentation in medical image analysis.
				  </div>
              </div>

				<!-- <div class="publication-item">
                <div class="publication-title">
                      <a href="https://www.frontiersin.org/journals/medicine/articles/10.3389/fmed.2021.699984/full">Can Clinical Symptoms and Laboratory Results Predict CT Abnormality? Initial Findings Using Novel Machine Learning Techniques in Children With COVID-19 Infections</a>
                  </div>
                  <div class="publication-authors">
                      Shaoping Hu, <strong>Yuan Gao</strong>, Zhangming Niu, Yinghui Jiang, Lao Li, Xianglu Xiao, Minhao Wang, Evandro Fei Fang, Wade Menpes-Smith, Jun Xia, Hui Ye, Guang Yang
                  </div>
                  <div class="publication-venue">Frontiers in Medicine, 2021</div>
                  <div class="publication-description">
                      We propose to ...
				  </div>
              </div> -->

              <!-- <div class="publication-item">
                  <div class="publication-title">
                      <a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Ye_Temporal_Cue_Guided_Video_Highlight_Detection_With_Low-Rank_Audio-Visual_Fusion_ICCV_2021_paper.pdf">Temporal Cue Guided Video Highlight Detection with Low-Rank Audio-Visual Fusion</a>
                  </div>
                  <div class="publication-authors">
                      Qinghao Ye, Xiyue Shen, <strong>Yuan Gao</strong>, Zirui Wang, Qi Bi, Ping Li, Guang Yang
                  </div>
                  <div class="publication-venue">ICCV, 2021</div>
                  <div class="publication-description">
                      We ... 
				  </div>
              </div> -->

              <!-- <div class="publication-item">
                  <div class="publication-title">
                      <a href="https://openaccess.thecvf.com/content/ICCV2021W/CVAMD/papers/Gao_A_Dual_Adversarial_Calibration_Framework_for_Automatic_Fetal_Brain_Biometry_ICCVW_2021_paper.pdf">A Dual Adversarial Calibration Framework for Automatic Fetal Brain Biometry</a>
                  </div>
                  <div class="publication-authors">
                      <strong>Yuan Gao</strong>, Lokhin Lee, Richard Droste, Rachel Craik, Sridevi Beriwal, Aris Papageorghiou, Alison Noble
                  </div>
                  <div class="publication-venue">ICCV, 2021</div>
                  <div class="publication-description">
                      Here, we propose 
			</div>
              </div> -->

              <!-- <div class="publication-item">
                  <div class="publication-title">
                      <a href="https://ora.ox.ac.uk/objects/uuid:ad7f8a0d-b81e-4df9-a20d-8522bc38fac5/files/srx913q301">Label Efficient Localization of Fetal Brain Biometry Planes In Ultrasound Through Metric Learning</a>
                  </div>
                  <div class="publication-authors">
                      <strong>Yuan Gao</strong>, Sridevi Beriwal, Rachel Craik, Aris T Papageorghiou, J Alison Noble
                  </div>
                  <div class="publication-venue">International Workshop on Advances in Simplifying Medical Ultrasound(ASMUS Workshop), 2020</div>
                  <div class="publication-description">
                      In this paper, we consider the problem of 
				  </div>
              </div> -->

				
              <div class="publication-item">
                  <div class="publication-title">
                      <a href="https://ora.ox.ac.uk/objects/uuid:617b7444-2c45-473b-b3e4-4dcf7eda9482/files/sth83kz34z">Learning and Understanding Deep Spatio-Temporal Representations from Free-Hand Fetal Ultrasound Sweeps</a>
                  </div>
                  <div class="publication-authors">
                      <strong>Yuan Gao</strong>, J Alison Noble
                  </div>
                  <div class="publication-venue">MICCAI, 2019</div>
                  <div class="publication-description">
                      We propose an end-to-end, weakly supervised method for identifying fetal structures in nonstandard ultrasound planes using video clips. By integrating convolutional LSTMs with an attention-gated mechanism, our model captures local temporal dynamics and generates spatio-temporal attention maps for structure localization—requiring only image-level labels. This approach significantly boosts both classification precision and localization accuracy.
				  </div>
              </div>

              <div class="publication-item">
                  <div class="publication-title">
                      <a href="https://ora.ox.ac.uk/objects/uuid:e41c62a8-bcf3-4ab9-bec5-6e3b885a2738/download_file?safe_filename=Gao-2019-Fetal-Heartbeat.pdf&file_format=application%2Fpdf&type_of_work=Conference+item">Detection and Characterization of the Fetal Heartbeat in Free-hand Ultrasound Sweeps with Weakly-supervised Two-streams Convolutional Networks</a>
                  </div>
                  <div class="publication-authors">
                      <strong>Yuan Gao</strong>, J Alison Noble
                  </div>
                  <div class="publication-venue">MICCAI, 2017</div>
                  <div class="publication-description">
                      We propose a compact, end-to-end two-stream ConvNet for weakly supervised detection and localization of beating fetal hearts in free-hand ultrasound videos. It leverages spatio-temporal features, enforces rotation invariance without extra augmentation, and is robust to artifacts like acoustic shadows. Our method significantly outperforms single-stream models (90.3% vs. 74.9% accuracy) in heart identification.
				  </div>
              </div>





             
            </td>
          </tr>
        </tbody></table>


		  
        </tbody>
		</table>
      </td>
    </tr>
  </table>
</body>

</html>

       
